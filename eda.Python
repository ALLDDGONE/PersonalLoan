# Load the data from the Apporto machine to the Colab environment

from google.colab import files
uploaded = files.upload()
# Pandas is the Python package for data frames

import pandas as pd
# Part 1 Data Acquisition

# Read data from a CSV file into a data frame
df = pd.read_csv('PersonalLoan.csv')

# Display the first ten rows of the data frame to examine if it is an individual-level data set
print(df.head(10))

# Display the variable list
print(df.columns.values)

# Display the number of rows and the number of columns in the data set to confirm the portrait shape
# The first element of the output is the number of rows and the second is the number of columns 
print(df.shape)
# Part 3 Missing Value Imputation

# Show the number of missing values for each variable in the data frame
df.isnull().sum()

# Drop ZIP Code for now
rvar_list =['ZIPCode']
df_sample1 = df.drop(columns=rvar_list)

# Separate all the variables into two lists for future column indexing
# One for numerical, the other for categorical 
cvar_list = ['Education', 'SecuritiesAccount', 'CDAccount', 'Online', 'CreditCard', 'PersonalLoan']
nvar_list = ['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Mortgage']

# Check if there is any missing value left
df_sample1.isnull().sum()
# Part 4 Variable Transformation

# Standardize the numerical variables 
df_sample2 = df_sample1.copy()
df_sample2[nvar_list] = (df_sample1[nvar_list] - df_sample1[nvar_list].mean())/df_sample1[nvar_list].std()

# Set the datatype for the variables in the cvar_list to be categorical in Python
# Set the datatype for the variables in the nvar_list to be numerical in Python 
df_sample3 = df_sample2.copy()
df_sample3[cvar_list] = df_sample2[cvar_list].astype('category')
df_sample3[nvar_list] = df_sample2[nvar_list].astype('float64')

# Convert the categorical variables into dummies (Step 1 of dummy coding)
# prefix_sep is the sympol used to create the dummy variable names.

df_sample4 = df_sample3.copy()
df_sample4 = pd.get_dummies(df_sample3, prefix_sep='_')

# Remove the redundant dummies (Step 2 of dummy coding)
# Placeholder variable: rdummies
rdummies = ['Education_1', 'SecuritiesAccount_Yes', 'CDAccount_Yes', 'Online_Yes', 'CreditCard_Yes', 'PersonalLoan_No']
df_sample5 = df_sample4.copy()
df_sample5 = df_sample4.drop(columns=rdummies)

# Get the remaining variable list after the variable transformation
print(df_sample5.columns.values)

# Display the milestone dataframe. Compare it with the original dataframe.
print(df_sample5)
print(df)
# Part 5 Data Partiton

# Required package: scikit-learn. Package name in Python: sklearn
# Required subpackage: model_selection. Required function name: train_test_split
from sklearn.model_selection import train_test_split

# Placeholder variables: df4partition, testpart_size
# test_size specifies the percentage for the test partition
df4partition = df_sample5
testpart_size = 0.2

# random_state specifies the seed for random number generator. 
# random_state = 1 unless otherwised noted
df_nontestData, df_testData = train_test_split(df4partition, test_size=testpart_size, random_state=1)

print(df_nontestData)
# Part 6 Logistic Regression with Penalty 

# Required package: scikit-learn. Package name in Python: sklearn
# Required subpackage: linear_model. 
# Required function name: LogisticRegression, LogisticRegressionCV

from sklearn.linear_model import LogisticRegression, LogisticRegressionCV

# Separate the predictor values and the DV values into X and y respectively
# Placeholder variable: DV
DV = 'PersonalLoan_Yes'
y = df_nontestData[DV]
X = df_nontestData.drop(columns=[DV])

# Run Logistic regression with pre-specified penalty level (alpha)
# Placeholder variable: alpha
alpha = 10

# The Logistic regression results are put into a model object clf
clf = LogisticRegression(C=1/alpha, penalty='l1', solver='saga', max_iter=200, random_state=1).fit(X,y)

# A user-defined function summary_coef
# to display the estimated coefficients of a model candidate obtained by the Logistic Regression analysis
def summary_coef(model_object):
  n_predictors = X.shape[1]
  model_coef = pd.DataFrame(model_object.coef_.reshape(1, n_predictors), columns=X.columns.values)
  model_coef['Intercept'] = model_object.intercept_
  return model_coef.transpose()

print(summary_coef(clf))

# Run Logistic regression with k-fold cross validation with k=5
# Placeholder variable: kfolds
kfolds = 5

# Here we specify within which range of the penalty levels we will search for the optimal penalty level, 
# i.e., the level that leads to the best model candidate
# We search the optimal alpha within [min_alpha, max_alpha]
min_alpha = 0.001
max_alpha = 100

# We further discretize the continuous alpha range [min_alpha, max_alpha] into n individual points of alpha
# We train n model candidates each of which corresponds to one individual alpha point
n_candidates = 1000

import numpy as np

# We store the list of individual alpha points into alpha_list
alpha_list = list(np.linspace(min_alpha, max_alpha, num=n_candidates))

# C_list is the element-wise inverse of alpha_list. It is required as one of the paramater values for LogisticRegressionCV
C_list = list(1/np.linspace(min_alpha, max_alpha, num=n_candidates))

# Set n_jobs to be -1 to run LogisticRegressionCV on all CPU cores.
clf_optimal = LogisticRegressionCV(Cs=C_list, cv=kfolds, penalty='l1', solver='saga', max_iter=200, random_state=1, n_jobs=-1).fit(X,y)

# Display the estimated coefficients of the final selected model
print(summary_coef(clf_optimal))

# Display the optimal alpha that yields the final selected model (the best model candidate)
print(1/clf_optimal.C_)

# Calcuate the error rate over the test partition based on the final selected model

# y_test_actual is the actual values of the DV in the test partition
y_test_actual = df_testData[DV]

# X_test is the predictor values in the test partition
X_test = df_testData.drop(columns=[DV])

# Use predict method of the clf_optimal object to apply the model associated with clf_optimal to the test partition
# y_test_predicted is the predicted values of the DV in the test partition 
y_test_predicted = clf_optimal.predict(X_test)

# Import the metrics package
from sklearn import metrics

# Display the confusion matrix over the test partition
print(metrics.confusion_matrix(y_test_actual, y_test_predicted))

# Display the accuracy over the test partition
print(clf_optimal.score(X_test, y_test_actual))
# Part7 Score the new data

# Upload the new data file from the local drive to Colab
from google.colab import files
uploaded = files.upload()
# Load the data from the new data file to a Python dataframe df_newdata
df_newdata = pd.read_csv('PersonalLoan_NEWDATA.csv')

# Check if there is any regular missing value in df_newdata
print(df_newdata.isnull().sum())
# Generate the categorical predictor list
Original_DV = 'PersonalLoan'
cpredictor_list = cvar_list.copy()
cpredictor_list.remove(Original_DV)

# Drop the redundant variable, e.g., ZIPCode as we did for the historical data
df_newdata_sample1 = df_newdata.drop(columns=rvar_list)

# Set the datatypes of the variables in the new data
df_newdata_sample2 = df_newdata_sample1.copy()
df_newdata_sample2[cpredictor_list] = df_newdata_sample1[cpredictor_list].astype('category')
df_newdata_sample2[nvar_list] = df_newdata_sample1[nvar_list].astype('float64')

# Derive the sample mean and the sample standard deviation of the numerical variables in the historical data 
historical_sample_mean = df_sample1[nvar_list].mean()
historical_sample_std = df_sample1[nvar_list].std()

# Use the historical sample mean and historical sample standard deviation to standardize the new data
df_newdata_sample3 = df_newdata_sample2.copy()
df_newdata_sample3[nvar_list] = (df_newdata_sample2[nvar_list] - historical_sample_mean[nvar_list])/historical_sample_std[nvar_list]

# Code the categorical variables in the new data 
df_newdata_sample4 = pd.get_dummies(df_newdata_sample3, prefix_sep='_')

# Fix the inconsistency between the predictors of the new data and the predictors required by the final selected model 
df_newdata_sample5 = df_newdata_sample4.copy()
df_newdata_sample5['Education_2'] = 0

print(df_newdata_sample5.columns.values)
# Score the new data using the model carried by the model object clf_optimal 
predicted_PersonalLoan = clf_optimal.predict_proba(df_newdata_sample5)[:,1]

# Print the predicted probablity for the new data observations
print(predicted_PersonalLoan)
#Variable Transformation #
#NA_Sales kNN
cvar_list = ['NA_Sales','Platform', 'Year_of_Release', 'Genre', 'Publisher', 'Rating', 'User_Score']
nvar_list = ['Critic_Score', 'Critic_Count', 'User_Count']
df_KNAsample = df_kNNsample5.copy()
df_KNAsample[nvar_list] = (df_kNNsample5[nvar_list] - df_kNNsample5[nvar_list].mean())/df_kNNsample5[nvar_list].std()

df_KNAsample[cvar_list] = df_kNNsample5[cvar_list].astype('category')
df_KNAsample[nvar_list] = df_kNNsample5[nvar_list].astype('float64')
df_KNAsample2 = df_KNAsample.copy()
df_KNAsample2 = pd.get_dummies(df_KNAsample, prefix_sep='_')

print(df_KNAsample[cvar_list].mode().loc[0:])

rdummies = ['NA_Sales_0']
df_KNAsample3 = df_KNAsample2.copy()
df_KNAsample3 = df_KNAsample2.drop(columns=rdummies)

other_area_sales=['EU_Sales_0','EU_Sales_1','JP_Sales_0','JP_Sales_1','Other_Sales_0', 'Other_Sales_1','Global_Sales_0','Global_Sales_1' ]
df_KNAsample4 = df_KNAsample3.copy()
df_KNAsample4 = df_KNAsample3.drop(columns=other_area_sales)
print(df_KNAsample4.columns.values)
